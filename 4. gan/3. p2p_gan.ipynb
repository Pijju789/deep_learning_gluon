{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn, utils\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "ctx = mx.gpu()\n",
    "data_dir = '/Users/air/python/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_dir+'/facades'\n",
    "train_img_path = dataset+'/train'\n",
    "val_img_path = dataset+'/val'\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "lambda1 = 100\n",
    "\n",
    "pool_size = 50\n",
    "\n",
    "img_wd = 256\n",
    "img_ht = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, batch_size, is_reversed=False):\n",
    "    if not os.path.exists(dataset):\n",
    "        url = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
    "        os.mkdir(dataset)\n",
    "        data_file = utils.download(url)\n",
    "        with tarfile.open(data_file) as tar:\n",
    "            tar.extractall(path='.')\n",
    "        os.remove(data_file)\n",
    "    img_in_list = []\n",
    "    img_out_list = []\n",
    "    for path, _, fnames in os.walk(path):\n",
    "        for fname in fnames:\n",
    "            if not fname.endswith('.jpg'):\n",
    "                continue\n",
    "            img = os.path.join(path, fname)\n",
    "            img_arr = mx.image.imread(img).astype(np.float32)/127.5 - 1\n",
    "            img_arr = mx.image.imresize(img_arr, img_wd * 2, img_ht)\n",
    "            img_arr_in, img_arr_out = [mx.image.fixed_crop(img_arr, 0, 0, img_wd, img_ht),\n",
    "                                       mx.image.fixed_crop(img_arr, img_wd, 0, img_wd, img_ht)]\n",
    "            img_arr_in, img_arr_out = [nd.transpose(img_arr_in, (2,0,1)), \n",
    "                                       nd.transpose(img_arr_out, (2,0,1))]\n",
    "            img_arr_in, img_arr_out = [img_arr_in.reshape((1,) + img_arr_in.shape), \n",
    "                                       img_arr_out.reshape((1,) + img_arr_out.shape)]\n",
    "            img_in_list.append(img_arr_out if is_reversed else img_arr_in)\n",
    "            img_out_list.append(img_arr_in if is_reversed else img_arr_out)\n",
    "    return mx.io.NDArrayIter(\n",
    "        data=[nd.concat(*img_in_list, dim=0), nd.concat(*img_out_list, dim=0)], \n",
    "        batch_size=batch_size)\n",
    "\n",
    "train_data = load_data(train_img_path, batch_size, is_reversed=True)\n",
    "val_data = load_data(val_img_path, batch_size, is_reversed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(img_arr):\n",
    "    plt.imshow(((img_arr.asnumpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetSkipUnit(HybridBlock):\n",
    "    def __init__(self, inner_channels, outer_channels, inner_block=None, innermost=False, outermost=False,\n",
    "                 use_dropout=False, use_bias=False):\n",
    "        super(UnetSkipUnit, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.outermost = outermost\n",
    "            en_conv = nn.Conv2D(channels=inner_channels, kernel_size=4, strides=2, padding=1,\n",
    "                             in_channels=outer_channels, use_bias=use_bias)\n",
    "            en_relu = nn.LeakyReLU(alpha=0.2)\n",
    "            en_norm = nn.BatchNorm(momentum=0.1, in_channels=inner_channels)\n",
    "            de_relu = nn.Activation(activation='relu')\n",
    "            de_norm = nn.BatchNorm(momentum=0.1, in_channels=outer_channels)\n",
    "            if innermost:\n",
    "                de_conv = nn.Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels, use_bias=use_bias)\n",
    "                encoder = [en_relu, en_conv]\n",
    "                decoder = [de_relu, de_conv, de_norm]\n",
    "                model = encoder + decoder\n",
    "            elif outermost:\n",
    "                de_conv = nn.Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels * 2)\n",
    "                encoder = [en_conv]\n",
    "                decoder = [de_relu, de_conv, nn.Activation(activation='tanh')]\n",
    "                model = encoder + [inner_block] + decoder\n",
    "            else:\n",
    "                de_conv = nn.Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels * 2, use_bias=use_bias)\n",
    "                encoder = [en_relu, en_conv, en_norm]\n",
    "                decoder = [de_relu, de_conv, de_norm]\n",
    "                model = encoder + [inner_block] + decoder\n",
    "            if use_dropout:\n",
    "                model += [nn.Dropout(rate=0.5)]\n",
    "            self.model = nn.HybridSequential()\n",
    "            with self.model.name_scope():\n",
    "                for block in model:\n",
    "                    self.model.add(block)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return F.concat(self.model(x), x, dim=1)\n",
    "\n",
    "class UnetGenerator(HybridBlock):\n",
    "    def __init__(self, in_channels, num_downs, ngf=64, use_dropout=True):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        unet = UnetSkipUnit(ngf * 8, ngf * 8, innermost=True)\n",
    "        for _ in range(num_downs - 5):\n",
    "            unet = UnetSkipUnit(ngf * 8, ngf * 8, unet, use_dropout=use_dropout)\n",
    "        unet = UnetSkipUnit(ngf * 8, ngf * 4, unet)\n",
    "        unet = UnetSkipUnit(ngf * 4, ngf * 2, unet)\n",
    "        unet = UnetSkipUnit(ngf * 2, ngf * 1, unet)\n",
    "        unet = UnetSkipUnit(ngf, in_channels, unet, outermost=True)\n",
    "        with self.name_scope():\n",
    "            self.model = unet\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(HybridBlock):\n",
    "    def __init__(self, in_channels, ndf=64, n_layers=3, use_sigmoid=False, use_bias=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.model = nn.HybridSequential()\n",
    "            kernel_size = 4\n",
    "            padding = int(np.ceil((kernel_size - 1)/2))\n",
    "            self.model.add(nn.Conv2D(channels=ndf, kernel_size=kernel_size, strides=2,\n",
    "                                  padding=padding, in_channels=in_channels))\n",
    "            self.model.add(nn.LeakyReLU(alpha=0.2))\n",
    "            nf_mult = 1\n",
    "            for n in range(1, n_layers):\n",
    "                nf_mult_prev = nf_mult\n",
    "                nf_mult = min(2 ** n, 8)\n",
    "                self.model.add(nn.Conv2D(channels=ndf * nf_mult, kernel_size=kernel_size, strides=2,\n",
    "                                      padding=padding, in_channels=ndf * nf_mult_prev,\n",
    "                                      use_bias=use_bias))\n",
    "                self.model.add(nn.BatchNorm(momentum=0.1, in_channels=ndf * nf_mult))\n",
    "                self.model.add(nn.LeakyReLU(alpha=0.2))\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n_layers, 8)\n",
    "            self.model.add(nn.Conv2D(channels=ndf * nf_mult, kernel_size=kernel_size, strides=1,\n",
    "                                  padding=padding, in_channels=ndf * nf_mult_prev,\n",
    "                                  use_bias=use_bias))\n",
    "            self.model.add(nn.BatchNorm(momentum=0.1, in_channels=ndf * nf_mult))\n",
    "            self.model.add(nn.LeakyReLU(alpha=0.2))\n",
    "            self.model.add(nn.Conv2D(channels=1, kernel_size=kernel_size, strides=1,\n",
    "                                  padding=padding, in_channels=ndf * nf_mult))\n",
    "            if use_sigmoid:\n",
    "                self.model.add(nn.Activation(activation='sigmoid'))\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_init(param):\n",
    "    if param.name.find('conv') != -1:\n",
    "        if param.name.find('weight') != -1:\n",
    "            param.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "        else:\n",
    "            param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "    elif param.name.find('batchnorm') != -1:\n",
    "        param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        if param.name.find('gamma') != -1:\n",
    "            param.set_data(nd.random_normal(1, 0.02, param.data().shape))\n",
    "\n",
    "def network_init(net):\n",
    "    for param in net.collect_params().values():\n",
    "        param_init(param)\n",
    "\n",
    "def set_network():\n",
    "    netG = UnetGenerator(in_channels=3, num_downs=8)\n",
    "    netD = Discriminator(in_channels=6)\n",
    "    network_init(netG)\n",
    "    network_init(netD)\n",
    "    trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "    trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "    return netG, netD, trainerG, trainerD\n",
    "\n",
    "GAN_loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "L1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "netG, netD, trainerG, trainerD = set_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        ret_imgs = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image = nd.expand_dims(images[i], axis=0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                ret_imgs.append(image)\n",
    "            else:\n",
    "                p = nd.random_uniform(0, 1, shape=(1,)).asscalar()\n",
    "                if p > 0.5:\n",
    "                    random_id = nd.random_uniform(0, self.pool_size - 1, shape=(1,)).astype(np.uint8).asscalar()\n",
    "                    tmp = self.images[random_id].copy()\n",
    "                    self.images[random_id] = image\n",
    "                    ret_imgs.append(tmp)\n",
    "                else:\n",
    "                    ret_imgs.append(image)\n",
    "        ret_imgs = nd.concat(*ret_imgs, dim=0)\n",
    "        return ret_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pool = ImagePool(pool_size)\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_data:\n",
    "        real_in = batch.data[0].as_in_context(ctx)\n",
    "        real_out = batch.data[1].as_in_context(ctx)\n",
    "        fake_out = netG(real_in)\n",
    "        fake_concat = image_pool.query(nd.concat(real_in, fake_out, dim=1))\n",
    "        with autograd.record():\n",
    "            output = netD(fake_concat)\n",
    "            fake_label = nd.zeros(output.shape, ctx=ctx)\n",
    "            errD_fake = GAN_loss(output, fake_label)\n",
    "            real_concat = nd.concat(real_in, real_out, dim=1)\n",
    "            output = netD(real_concat)\n",
    "            real_label = nd.ones(output.shape, ctx=ctx)\n",
    "            errD_real = GAN_loss(output, real_label)\n",
    "            errD = (errD_real + errD_fake) * 0.5\n",
    "            errD.backward()\n",
    "        trainerD.step(batch.data[0].shape[0])\n",
    "        with autograd.record():\n",
    "            fake_out = netG(real_in)\n",
    "            fake_concat = nd.concat(real_in, fake_out, dim=1)\n",
    "            output = netD(fake_concat)\n",
    "            real_label = nd.ones(output.shape, ctx=ctx)\n",
    "            errG = GAN_loss(output, real_label) + L1_loss(real_out, fake_out) * lambda1\n",
    "            errG.backward()\n",
    "        trainerG.step(batch.data[0].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
