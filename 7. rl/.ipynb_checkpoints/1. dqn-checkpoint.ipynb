{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import os, random, gym, math\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "f = open('results.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        #Architecture\n",
    "        self.batch_size = 32 # The size of the batch to learn the Q-function\n",
    "        self.image_size = 84 # Resize the raw input frame to square frame of size 80 by 80 \n",
    "        #Trickes\n",
    "        self.replay_buffer_size = 1000000 # The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "        self.learning_frequency = 4 # With Freq of 1/4 step update the Q-network\n",
    "        self.skip_frame = 4 # Skip 4-1 raw frames between steps\n",
    "        self.internal_skip_frame = 4 # Skip 4-1 raw frames between skipped frames\n",
    "        self.frame_len = 4 # Each state is formed as a concatination 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "        self.Target_update = 10000 # Update the target network each 10000 steps\n",
    "        self.epsilon_min = 0.1 # Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "        self.annealing_end = 1000000. # The number of step it take to linearly anneal the epsilon to it min value\n",
    "        self.gamma = 0.99 # The discount factor\n",
    "        self.replay_start_size = 50000 # Start to backpropagated through the network, learning starts\n",
    "        self.no_op_max = 30 / self.skip_frame # Run uniform policy for first 30 times step of the beginning of the game\n",
    "        \n",
    "        #otimization\n",
    "        self.num_episode = 150 # Number episode to run the algorithm\n",
    "        self.lr = 0.00025 # RMSprop learning rate\n",
    "        self.gamma1 = 0.95 # RMSprop gamma1\n",
    "        self.gamma2 = 0.95 # RMSprop gamma2\n",
    "        self.rms_eps = 0.01 # RMSprop epsilon bias\n",
    "        self.ctx = mx.gpu() # Enables gpu if available, if not, set it to mx.cpu()\n",
    "opt = Options()\n",
    "\n",
    "env_name = 'AssaultNoFrameskip-v4' # Set the desired environment\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n # Extract the number of available action from the environment setting\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)\n",
    "attrs = vars(opt)\n",
    "print (', '.join(\"%s: %s\" % item for item in attrs.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
